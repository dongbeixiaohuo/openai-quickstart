{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d81c83-4083-4ec1-ac4c-e96840024278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helloworld\n"
     ]
    }
   ],
   "source": [
    "print (\"helloworld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a5b694-e332-4311-a20f-4e55fc9e3cb3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /home/toy/.local/lib/python3.10/site-packages (0.2.16)\n",
      "Requirement already satisfied: langchain_community in /home/toy/.local/lib/python3.10/site-packages (0.2.16)\n",
      "Requirement already satisfied: langchain_chroma in /home/toy/.local/lib/python3.10/site-packages (0.1.3)\n",
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.1.23-py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/toy/.local/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/toy/.local/lib/python3.10/site-packages (from langchain) (0.1.117)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/toy/.local/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/toy/.local/lib/python3.10/site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/toy/.local/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/toy/.local/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/toy/.local/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/toy/.local/lib/python3.10/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /home/toy/.local/lib/python3.10/site-packages (from langchain) (0.2.38)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/toy/.local/lib/python3.10/site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/toy/.local/lib/python3.10/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from langchain_chroma) (0.112.2)\n",
      "Requirement already satisfied: chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 in /home/toy/.local/lib/python3.10/site-packages (from langchain_chroma) (0.5.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.42.0)\n",
      "Collecting tiktoken<1,>=0.7\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /home/toy/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/toy/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/toy/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/toy/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/toy/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.2.0)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.7.3)\n",
      "Requirement already satisfied: build>=1.0.3 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.66.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.19.2)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.30.6)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.2.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.27.0)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (7.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.12.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.6.4)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48.9)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.12.5)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48b0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (30.1.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.20.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.10.7)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.27.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/toy/.local/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.1.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (6.4.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/toy/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/toy/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1,>=0.95.2->langchain_chroma) (0.38.3)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (24.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/toy/.local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/toy/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/toy/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.5/776.5 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/toy/.local/lib/python3.10/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.0.1)\n",
      "Requirement already satisfied: pyproject_hooks in /home/toy/.local/lib/python3.10/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.1.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/lib/python3/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (2.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/toy/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-oauthlib in /home/toy/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.0.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/toy/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.34.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/lib/python3/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers in /home/toy/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (24.3.25)\n",
      "Requirement already satisfied: sympy in /home/toy/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.13.2)\n",
      "Requirement already satisfied: coloredlogs in /home/toy/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (15.0.1)\n",
      "Requirement already satisfied: protobuf in /home/toy/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.25.4)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/toy/.local/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /home/toy/.local/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/toy/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in /home/toy/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /home/toy/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /home/toy/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /home/toy/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /home/toy/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /home/toy/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/toy/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (74.0.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /home/toy/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/toy/.local/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.2.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/toy/.local/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.24.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (13.8.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/lib/python3/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/toy/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/toy/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.20.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (12.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/toy/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/lib/python3/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.19.2)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/toy/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.24.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/toy/.local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/toy/.local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.2.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2024.6.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.15.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.18.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/toy/.local/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/toy/.local/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.1.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.4.8)\n",
      "Installing collected packages: regex, tiktoken, langchain_openai\n",
      "Successfully installed langchain_openai-0.1.23 regex-2024.7.24 tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_community langchain_chroma langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd72581",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd5318a1-2bb9-45bb-8206-82fcf024c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import bs4\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-UtASYrjftCMXZhV36fC14c4b65394dF494640d2e5538E53e\"\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23f36b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "连接成功！\n",
      "嵌入向量的前 5 个元素:\n",
      "[-0.005830363, -0.024080755, -0.02201979, 0.02761384, -0.0027602217]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# 设置 API 密钥和基础 URL\n",
    "base_url = \"https://api.javis3000.com/v1\"\n",
    "\n",
    "# 设置请求头\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# 准备请求数据\n",
    "data = {\n",
    "    \"input\": \"Hello, world!\",\n",
    "    \"model\": \"text-embedding-3-large\"\n",
    "}\n",
    "\n",
    "# 发送请求\n",
    "try:\n",
    "    response = requests.post(f\"{base_url}/embeddings\", headers=headers, json=data)\n",
    "    \n",
    "    # 检查响应状态\n",
    "    if response.status_code == 200:\n",
    "        print(\"连接成功！\")\n",
    "        result = response.json()\n",
    "        print(\"嵌入向量的前 5 个元素:\")\n",
    "        print(result['data'][0]['embedding'][:5])\n",
    "    else:\n",
    "        print(f\"错误: HTTP 状态码 {response.status_code}\")\n",
    "        print(\"响应内容:\")\n",
    "        print(json.dumps(response.json(), indent=2))\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"请求异常: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ccef98-31df-4a7d-a5d1-94785651e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 WebBaseLoader 从网页加载内容，并仅保留标题、标题头和文章内容\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"h3-mktg lh-condensed mb-3 color-fg-default\", \"d-flex flex-items-center mb-6px\", \"d-flex flex-wrap flex-justify-center gutter-spacious\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://github.blog/ai-and-ml/generative-ai/what-is-retrieval-augmented-generation-and-what-does-it-do-for-generative-ai/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e5c2d11-eff2-43d1-8800-9d9d3a301f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12210\n"
     ]
    }
   ],
   "source": [
    "# 检查加载的文档内容长度\n",
    "print(len(docs[0].page_content))  # 打印第一个文档内容的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94fd046-6a2a-4761-a83b-a9f82ed53b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is retrieval-augmented generation, and what does it do for generative AI?\n",
      "\n",
      "\n",
      "Nicole Choi·@nicchoi29 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "One of the hottest topics in AI right now is RAG, or retrieval-augmented generation, which is a retrieval method used by some AI tools to improve the quality and relevance of their outputs.\n",
      "Organizations want AI tools that use RAG because it makes those tools aware of proprietary data without the effort and expense of custom model training. RAG also keeps models up to date.  When generati\n"
     ]
    }
   ],
   "source": [
    "# 查看第一个文档（前100字符）\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0958c3b5-4110-44b2-9e27-4a82015c0a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 RecursiveCharacterTextSplitter 将文档分割成块，每块1000字符，重叠200字符\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d73830-8e73-4293-80b0-7e631f7344d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "# 检查分割后的块数量和内容\n",
    "print(len(all_splits))  # 打印分割后的文档块数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5263f892-4a80-42b9-b79e-ac3e303a60ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "print(len(all_splits[0].page_content))  # 打印第一个块的字符数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46183c0c-91ec-4609-b7c3-e85c08d8d975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is retrieval-augmented generation, and what does it do for generative AI?\n",
      "\n",
      "\n",
      "Nicole Choi·@nicchoi29\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[0].page_content)  # 打印第一个块的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "034ce612-96d0-4b61-8d53-884b48a85a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://github.blog/ai-and-ml/generative-ai/what-is-retrieval-augmented-generation-and-what-does-it-do-for-generative-ai/', 'language': 'No language found.', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[0].metadata)  # 打印第一个块的元数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96fb11-41cb-4d79-b7ec-cc294187baf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226e67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"请设置OPENAI_API_KEY环境变量\")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    base_url=\"https://api.javis3000.com/v1\",\n",
    "    model=\"text-embedding-3-large\",\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fddd491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试嵌入的长度: 3072\n",
      "测试嵌入的前5个元素: [-0.005830362904816866, -0.024080755189061165, -0.02201979048550129, 0.027613839134573936, -0.002760221716016531]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_embedding = embeddings.embed_query(\"Hello, world!\")\n",
    "    print(f\"测试嵌入的长度: {len(test_embedding)}\")\n",
    "    print(f\"测试嵌入的前5个元素: {test_embedding[:5]}\")\n",
    "except Exception as e:\n",
    "    print(f\"嵌入测试失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8527ffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain 版本: 0.2.16\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(f\"LangChain 版本: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c505ed80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Insert of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 3\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.get_or_create_collection(\"collection_name\")\n",
    "collection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n",
    "\n",
    "vector_store_from_client = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"collection_name\",\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef288a05-25a4-4f2f-827a-a5f318ff0d7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-UtASY***************************************E53e. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 使用 Chroma 向量存储和 OpenAIEmbeddings 模型，将分割的文档块嵌入并存储\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_chroma/vectorstores.py:1128\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m   1127\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m-> 1128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_chroma/vectorstores.py:1089\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[1;32m   1084\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[1;32m   1085\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1087\u001b[0m         )\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1089\u001b[0m     \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_chroma/vectorstores.py:508\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_openai/embeddings/base.py:616\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 616\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_openai/embeddings/base.py:514\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    512\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 514\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    518\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py:1260\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1248\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1255\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1257\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1258\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1259\u001b[0m     )\n\u001b[0;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py:937\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    930\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    936\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py:1041\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1040\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1044\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1045\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1049\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1050\u001b[0m )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-UtASY***************************************E53e. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# 使用 Chroma 向量存储和 OpenAIEmbeddings 模型，将分割的文档块嵌入并存储\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e872ff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量存储创建成功\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "try:\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"example_collection\",\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=\"./chroma_langchain_db\",\n",
    "    )\n",
    "    print(\"向量存储创建成功\")\n",
    "except Exception as e:\n",
    "    print(f\"创建向量存储失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0417b021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功添加测试文档到向量存储\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vector_store.add_texts([\"这是第一个测试文档\", \"这是第二个测试文档\"])\n",
    "    print(\"成功添加测试文档到向量存储\")\n",
    "except Exception as e:\n",
    "    print(f\"添加测试文档失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb0ede14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索结果: [Document(page_content='这是第一个测试文档')]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    results = vector_store.similarity_search(\"测试文档\", k=1)\n",
    "    print(\"搜索结果:\", results)\n",
    "except Exception as e:\n",
    "    print(f\"搜索失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3c37e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功添加 18 个文档到向量存储\n"
     ]
    }
   ],
   "source": [
    "if 'all_splits' in locals() and all_splits:\n",
    "    try:\n",
    "        vector_store.add_documents(all_splits)\n",
    "        print(f\"成功添加 {len(all_splits)} 个文档到向量存储\")\n",
    "    except Exception as e:\n",
    "        print(f\"添加文档失败: {e}\")\n",
    "else:\n",
    "    print(\"all_splits 未定义或为空\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39aa3545-b456-473a-b620-5fb0eab15dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_chroma.vectorstores.Chroma"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 vectorstore 数据类型\n",
    "type(vector_store) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b52313-646a-4512-8125-68054c1c7e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6c587ce-b40f-47ad-bd14-480c49a073e7",
   "metadata": {},
   "source": [
    "### Step 4: 检索文档\n",
    "\n",
    "- **描述**: 使用 `VectorStoreRetriever` 类的 `as_retriever()` 和 `invoke()` 方法，从向量数据库中检索与查询最相关的文档片段。\n",
    "- **重要代码抽象**:\n",
    "  - 类: `VectorStoreRetriever`\n",
    "  - 方法: `as_retriever()`, `invoke()`\n",
    "- **代码解释**:\n",
    "  - **文档检索**: 将向量存储转换为检索器，并基于查询执行相似性搜索，获取相关文档片段。\n",
    "  - **检查检索数量**: 打印检索到的文档片段数量，确保检索操作成功。\n",
    "  - **验证检索内容**: 输出第一个检索到的文档内容，确认检索结果与预期相符。\n",
    "\n",
    "在 LangChain 中，所有向量数据库都支持**vectorstore.as_retriever** 方法，实例化该数据库对应的检索器（Retriever），数据类型为`VectorStoreRetriever`，详细 API 文档：https://python.langchain.com/v0.2/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStoreRetriever.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cdab30e-b07d-4273-b734-3aabff356bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 VectorStoreRetriever 从向量存储中检索与查询最相关的文档\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6feddb48-d07e-40be-a9fe-790c3bb75d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.vectorstores.base.VectorStoreRetriever"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9283b2e6-874f-4815-915b-e87da0429a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"什么是RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d292f13-9581-4f28-b54d-ba0fe9fa0040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# 检查检索到的文档内容\n",
    "print(len(retrieved_docs))  # 打印检索到的文档数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "458d6a7d-21a9-49a4-b523-651e3bab51e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG can also retrieve information from external and internal search engines. When integrated with an external search engine, RAG can search and retrieve information from the entire internet. When integrated with an internal search engine, it can also access information from within your organization, like an internal website or platform. Integrating both kinds of search engines supercharges RAG’s ability to provide relevant responses.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)  # 打印第一个检索到的文档内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3309c1e-c650-44ae-baba-6f7528372931",
   "metadata": {},
   "source": [
    "### Step 5: 生成回答\n",
    "\n",
    "- **描述**: 将之前构建的组件（检索器、提示、LLM等）组合成一个完整的链条，实现用户问题的检索与生成回答。完整链条：输入用户问题，检索相关文档，构建提示，将其传递给模型（使用`ChatOpenAI` 类的 `invoke()` 方法），并解析输出生成最终回答。\n",
    "- **重要代码抽象**:\n",
    "  - 类: `ChatOpenAI`\n",
    "  - 方法: `invoke()`\n",
    "  - 类: `RunnablePassthrough`\n",
    "  - 类: `StrOutputParser`\n",
    "  - 模块：`hub`\n",
    "- **代码解释**:\n",
    "  - **模型初始化**: 使用 `ChatOpenAI` 类初始化一个 `GPT-4o-mini` 模型，准备处理生成任务。\n",
    "  - **文档格式化**: 定义 `format_docs` 函数，用于将检索到的文档内容格式化为字符串。\n",
    "  - **构建 RAG 链**: 使用 LCEL (LangChain Execution Layer) 的 `|` 操作符将各个组件连接成一个链条，包括文档检索、提示构建、模型调用以及输出解析。\n",
    "  - **生成回答**: 使用 `stream()` 方法逐步输出生成的回答，并实时展示，确保生成的结果符合预期。\n",
    "\n",
    "![retrieval](../images/retrieval.png)\n",
    "\n",
    "#### LangChain Hub\n",
    "\n",
    "`LangChain Hub` (https://smith.langchain.com/hub) 是一个提示词模板开源社区，为开发者提供了大量开箱即用的提示词模板。属于 `LangSmith` 产品的一部分。\n",
    "\n",
    "下面我们尝试使用 RAG 应用的提示词模板：https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "\n",
    "\n",
    "```\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3dbe88b-7a8f-4607-8426-698a3adf3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 RAG 链，将用户问题与检索到的文档结合并生成答案\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "488e8c40-a304-4538-a84e-cc8e4e7e101a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toy/.local/lib/python3.10/site-packages/langsmith/client.py:312: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "/home/toy/.local/lib/python3.10/site-packages/langsmith/client.py:5519: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n"
     ]
    }
   ],
   "source": [
    "# 使用 hub 模块拉取 rag 提示词模板\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf0de4c1-e389-430b-a472-521960e12b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
     ]
    }
   ],
   "source": [
    "# 打印模板\n",
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "281d2ea1-0a61-45aa-810d-6b80a7d102a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为 context 和 question 填充样例数据，并生成 ChatModel 可用的 Messages\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e55e098-5f2d-4ef6-98e2-b1a4f27fa853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# 查看提示词\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300cbbc6-aaaa-4f8c-8382-1cbda08d2fe9",
   "metadata": {},
   "source": [
    "#### ⭐️**LCEL 在 RAG 中的应用**⭐️\n",
    "\n",
    "##### **LCEL 概述**\n",
    "\n",
    "LCEL 是 LangChain 中的一个重要概念，它提供了一种统一的接口，允许不同的组件（如 `retriever`, `prompt`, `llm` 等）可以通过统一的 `Runnable` 接口连接起来。每个 `Runnable` 组件都实现了相同的方法，如 `.invoke()`、`.stream()` 或 `.batch()`，这使得它们可以通过 `|` 操作符轻松连接。\n",
    "\n",
    "##### **LCEL 中处理的组件**\n",
    "\n",
    "- **Retriever**: 负责根据用户问题检索相关文档。\n",
    "- **Prompt**: 根据检索到的文档构建提示，供模型生成回答。\n",
    "- **LLM**: 接收提示并生成最终的回答。\n",
    "- **StrOutputParser**: 解析 LLM 的输出，只提取字符串内容，供最终显示。\n",
    "\n",
    "##### **LCEL 运作机制**\n",
    "\n",
    "- **构建链条**: 通过 `|` 操作符，我们可以将多个 `Runnable` 组件连接成一个 `RunnableSequence`。LangChain 会自动将一些对象转换为 `Runnable`，如将 `format_docs` 转换为 `RunnableLambda`，将包含 `\"context\"` 和 `\"question\"` 键的字典转换为 `RunnableParallel`。\n",
    "\n",
    "- **数据流动**: 用户输入的问题会在 `RunnableSequence` 中依次经过各个 `Runnable` 组件。首先，问题会通过 `retriever` 检索相关文档，然后通过 `format_docs` 将这些文档转换为字符串。`RunnablePassthrough` 则直接传递原始问题。最后，这些数据被传递给 `prompt` 来生成完整的提示，供 LLM 使用。\n",
    "\n",
    "##### **LCEL 中的关键操作**\n",
    "\n",
    "- **格式化文档**: `retriever | format_docs` 将问题传递给 `retriever` 生成文档对象，然后通过 `format_docs` 将这些文档格式化为字符串。\n",
    "- **传递问题**: `RunnablePassthrough()` 直接传递原始问题，保持原样。\n",
    "- **构建提示**: `{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt` 构建完整的提示。\n",
    "- **运行模型**: `prompt | llm | StrOutputParser()` 运行 LLM 生成回答，并解析输出。\n",
    "\n",
    "#### 使用 LCEL 构建 RAG Chain\n",
    "\n",
    "下面我们将 LCEL 的概念与代码实现结合起来，展示了如何通过一系列 `Runnable` 组件来实现完整的 RAG 流程。通过 LCEL，LangChain 提供了高度模块化和可扩展的开发方式，使复杂任务的实现变得更加简单和高效。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5120936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 确保使用正确的 API 基础 URL 和密钥\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",  # 请确认这是正确的模型名称\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=\"https://api.javis3000.com/v1\"\n",
    ")\n",
    "\n",
    "# 定义提示模板\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the following question based on the context: {context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "# 定义格式化文档的函数\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# 重新定义 rag_chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fe460ec-547e-4714-a7dd-bf9dc319b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义格式化文档的函数\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66e6049b-af4d-435d-aded-68d9c80803bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 LCEL 构建 RAG Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e2880d17-79ac-4e5c-966b-144d9ee0dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG，即检索增强生成（retrieval-augmented generation），是一种用于一些人工智能工具的检索方法，用以提高其输出的质量和相关性。通过使用RAG，AI工具可以在无需进行定制模型训练的情况下访问专有数据，并保持模型的最新状态。例如，GitHub Copilot Chat在回答开发者问题时，会使用内部搜索引擎从索引文件中找到相关代码或文本片段，并通过语义搜索来检索最相关的内容，从而生成更加相关的回复。RAG不仅可以从外部搜索引擎检索信息，也可以从内部搜索引擎中获取企业内部的信息，极大地增强了AI模型的上下文理解和响应能力。"
     ]
    }
   ],
   "source": [
    "# 流式生成回答\n",
    "for chunk in rag_chain.stream(\"What is RAG，用中文回答?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb6b9b0e-9883-4375-8bc1-ea31f3659172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG（检索增强生成）的重要性在于它能够显著提高AI工具生成输出的质量和相关性。其关键优势包括：\n",
      "\n",
      "1. **利用专有数据**：RAG使AI工具可以访问组织内部的专有数据，而无需进行昂贵且耗时的自定义模型训练。这使得模型能够更加了解特定组织的特殊需求和数据。\n",
      "\n",
      "2. **保持模型的最新性**：传统模型只能基于训练时的数据生成答案，而RAG允许模型利用最新的私有数据库，提供更为准确和及时的信息。这对于快速变化的领域尤为重要。\n",
      "\n",
      "3. **扩展数据源**：RAG不仅可以使用模型原有的训练数据，还能整合来自外部和内部搜索引擎的数据源，从而大大增强生成输出的全面性和准确性。\n",
      "\n",
      "4. **减少定制化成本**：与需要调整模型权重的微调（fine-tuning）方法相比，RAG不需要对模型进行大量定制，这在节约时间和资源方面具有明显优势。\n",
      "\n",
      "综合这些因素，RAG成为当前AI领域的热门话题，并受到各组织的广泛关注和采用。"
     ]
    }
   ],
   "source": [
    "# 流式生成回答\n",
    "for chunk in rag_chain.stream(\"RAG的重要性？\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecf485cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Retrieval-Augmented Generation) 有多个重要用途：\n",
      "\n",
      "1. **提高生成内容的质量和相关性**：RAG通过检索相关信息来增强AI工具的输出质量和相关性。\n",
      "\n",
      "2. **节省定制模型训练的成本和精力**：RAG使AI工具能够利用专有数据，而无需进行昂贵和耗时的定制模型训练。\n",
      "\n",
      "3. **保持模型的最新性**：没有RAG的情况下，模型只能基于训练时的已有数据生成答案。而通过RAG，模型能够利用私人数据库中的最新信息，从而生成更为准确和及时的响应。\n",
      "\n",
      "4. **结合内部和外部搜索引擎**：RAG可以与内部和外部搜索引擎集成，从而在整个互联网和组织内部平台中检索信息，大大增强了RAG提供相关响应的能力。\n",
      "\n",
      "5. **语义搜索和排名**：在具体应用中，例如GitHub Copilot Chat，RAG使用语义搜索来分析和排名文档内容，从而找到最相关的代码或文本片段，生成开发者所需的答案。\n",
      "\n",
      "总之，RAG为定制AI模型提供了一种有效的方法，确保输出内容与组织知识、最佳实践和最新互联网信息保持一致。"
     ]
    }
   ],
   "source": [
    "# 流式生成回答\n",
    "for chunk in rag_chain.stream(\"RAG有啥用？\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a072d01-60b2-46c7-a104-c8a4a3eda0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19c3d53d-6ae2-4e05-878b-6702c10995bd",
   "metadata": {},
   "source": [
    "# Homework\n",
    "1. 使用其他的线上文档或离线文件，重新构建向量数据库，尝试提出3个相关问题，测试 LCEL 构建的 RAG Chain 是否能成功召回。\n",
    "2. 重新设计或在 LangChain Hub 上找一个可用的 RAG 提示词模板，测试对比两者的召回率和生成质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d91a2d-56e0-4f52-b6a3-dc893583b51f",
   "metadata": {},
   "source": [
    "### 自定义 Prompt 的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94c5d89d-d642-41cb-ab6f-6ead404b6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 自定义提示词模板\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40554af6",
   "metadata": {},
   "source": [
    "从hub上导入新的模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a00c7454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toy/.local/lib/python3.10/site-packages/langsmith/client.py:312: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "newprompt = hub.pull(\"pwoc517/more-crafted-rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "985266bc-1239-45b1-ab76-d7bd2f279cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Your role\n",
      "You are a brilliant expert at understanding the intent of the questioner and the crux of the question, and providing the most optimal answer to the questioner's needs from the documents you are given.\n",
      "\n",
      "\n",
      "# Instruction\n",
      "Your task is to answer the question using the following pieces of retrieved context delimited by XML tags.\n",
      "\n",
      "<retrieved context>\n",
      "Retrieved Context:\n",
      "filler context\n",
      "</retrieved context>\n",
      "\n",
      "\n",
      "# Constraint\n",
      "1. Think deeply and multiple times about the user's question\\nUser's question:\\nfiller question\\nYou must understand the intent of their question and provide the most appropriate answer.\n",
      "- Ask yourself why to understand the context of the question and why the questioner asked it, reflect on it, and provide an appropriate response based on what you understand.\n",
      "2. Choose the most relevant content(the key content that directly relates to the question) from the retrieved context and use it to generate an answer.\n",
      "3. Generate a concise, logical answer. When generating the answer, Do Not just list your selections, But rearrange them in context so that they become paragraphs with a natural flow. \n",
      "4. When you don't have retrieved context for the question or If you have a retrieved documents, but their content is irrelevant to the question, you should answer 'I can't find the answer to that question in the material I have'.\n",
      "5. Use five sentences maximum. Keep the answer concise but logical/natural/in-depth.\n",
      "\n",
      "\n",
      "# Question:\n",
      "filler question\n"
     ]
    }
   ],
   "source": [
    "# 为 context 和 question 填充样例数据，生成 LLM 可用的提示词\n",
    "print(newprompt.invoke({\"context\": \"filler context\", \"question\": \"filler question\"}).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fbe82ee-de48-4cc8-875c-d74242ef273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新自定义 RAG Chain\n",
    "custom_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | newprompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b818824-5a44-461a-b6d5-3ff079beaaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG（检索增强生成）与其他技术的主要区别在于它能够超越模型训练数据的限制，使用外部和内部数据源来生成输出。传统的生成式AI只能引用训练时期存在的信息，而RAG可以从整个互联网或组织内部的数据库中检索最新的信息，从而提供更及时和相关的响应。此外，RAG无需像微调那样调整模型权重，避免了高成本和复杂的定制过程。通过这种方式，RAG使得AI工具能够利用专有数据，而不需要进行昂贵的定制模型训练，同时保持模型的最新状态。'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用自定义 prompt 生成回答\n",
    "custom_rag_chain.invoke(\"RAG与其他技术差别?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6b1fa5d-674d-4e1c-a423-303aea90415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_prompt = ChatPromptTemplate.from_template(\n",
    "    \"你是一个Python专家。请根据以下功能需求,生成相应的Python代码实现:\\n\\n{requirement}\\n\\n请只输出代码,不要有任何解释。\"\n",
    ")\n",
    "\n",
    "java_prompt = ChatPromptTemplate.from_template(\n",
    "    \"你是一个Java专家。请根据以下功能需求,生成相应的Java代码实现:\\n\\n{requirement}\\n\\n请只输出代码,不要有任何解释。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1ceb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_chain = python_prompt | llm | StrOutputParser()\n",
    "java_chain = java_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75f921b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_code_chain = RunnableParallel(\n",
    "    requirement=RunnablePassthrough(),\n",
    "    python_code=python_chain,\n",
    "    java_code=java_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0a9ad82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python代码实现:\n",
      "```python\n",
      "def add(x, y):\n",
      "    return x + y\n",
      "\n",
      "def subtract(x, y):\n",
      "    return x - y\n",
      "\n",
      "def multiply(x, y):\n",
      "    return x * y\n",
      "\n",
      "def divide(x, y):\n",
      "    if y == 0:\n",
      "        return \"Error! Division by zero.\"\n",
      "    return x / y\n",
      "\n",
      "def calculator():\n",
      "    print(\"Select operation:\")\n",
      "    print(\"1. Add\")\n",
      "    print(\"2. Subtract\")\n",
      "    print(\"3. Multiply\")\n",
      "    print(\"4. Divide\")\n",
      "\n",
      "    choice = input(\"Enter choice(1/2/3/4): \")\n",
      "\n",
      "    if choice in ['1', '2', '3', '4']:\n",
      "        num1 = float(input(\"Enter first number: \"))\n",
      "        num2 = float(input(\"Enter second number: \"))\n",
      "\n",
      "        if choice == '1':\n",
      "            print(f\"{num1} + {num2} = {add(num1, num2)}\")\n",
      "\n",
      "        elif choice == '2':\n",
      "            print(f\"{num1} - {num2} = {subtract(num1, num2)}\")\n",
      "\n",
      "        elif choice == '3':\n",
      "            print(f\"{num1} * {num2} = {multiply(num1, num2)}\")\n",
      "\n",
      "        elif choice == '4':\n",
      "            result = divide(num1, num2)\n",
      "            if result == \"Error! Division by zero.\":\n",
      "                print(result)\n",
      "            else:\n",
      "                print(f\"{num1} / {num2} = {result}\")\n",
      "    else:\n",
      "        print(\"Invalid input\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    calculator()\n",
      "```\n",
      "\n",
      "Java代码实现:\n",
      "```java\n",
      "import java.util.Scanner;\n",
      "\n",
      "public class SimpleCalculator {\n",
      "\n",
      "    public static void main(String[] args) {\n",
      "        Scanner scanner = new Scanner(System.in);\n",
      "        System.out.println(\"Enter first number: \");\n",
      "        double num1 = scanner.nextDouble();\n",
      "        \n",
      "        System.out.println(\"Enter operation (+, -, *, /): \");\n",
      "        char operation = scanner.next().charAt(0);\n",
      "        \n",
      "        System.out.println(\"Enter second number: \");\n",
      "        double num2 = scanner.nextDouble();\n",
      "        \n",
      "        double result = 0;\n",
      "        \n",
      "        switch (operation) {\n",
      "            case '+':\n",
      "                result = num1 + num2;\n",
      "                break;\n",
      "            case '-':\n",
      "                result = num1 - num2;\n",
      "                break;\n",
      "            case '*':\n",
      "                result = num1 * num2;\n",
      "                break;\n",
      "            case '/':\n",
      "                if (num2 != 0) {\n",
      "                    result = num1 / num2;\n",
      "                } else {\n",
      "                    System.out.println(\"Error: Division by zero\");\n",
      "                    return;\n",
      "                }\n",
      "                break;\n",
      "            default:\n",
      "                System.out.println(\"Error: Invalid operation\");\n",
      "                return;\n",
      "        }\n",
      "        \n",
      "        System.out.println(\"The result is: \" + result);\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "requirement = \"实现一个简单的计算器,可以进行加减乘除四则运算\"\n",
    "\n",
    "result = multi_code_chain.invoke(requirement)\n",
    "\n",
    "print(\"Python代码实现:\")\n",
    "print(result[\"python_code\"])\n",
    "print(\"\\nJava代码实现:\")\n",
    "print(result[\"java_code\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
